#!/usr/bin/perl -s

use warnings;
use WWW::Mechanize;
#use Data::Dumper;
# Variables we can tweak to change behaviour
my $DEBUG = 1;
my $MAXDEPTH = 10;

my $string = shift(@ARGV);
my @URLS;
my %visited = ();
my $mech = WWW::Mechanize->new();

# Populate the initial URL list
foreach my $url (@ARGV) {
    push @URLS, [$url, 0];
}

while (my ($url, $depth) = shift(@URLS)) {
    crawl($string, $url, $depth);
       }

sub crawl {
    my ($string, $url, $depth) = shift;

    # Are we at the max depth yet?
    if ($depth >= $MAXDEPTH) {
	print "*** At Maximum depth: $url\n" if $DEBUG;
	return;
    }

    # Check it's not on the kill list
    if ($visited{$url}) {
	print "*** Already visited $url\n" if $DEBUG;
	return;
    } else {
	$visited{$url} = 1;
    }

    $mech->get($url);

    # See if we match the query string
    my $matches = ($mech->content() =~ m/$string/i);

    if ( $DEBUG || ($matches > 0)) {
	print "$url, $matches\n";
    }

    # Add all the links to the URL list

    foreach my $link ($mech->links()) {
	push @URLS, ($link->url(), $depth+1);
    }
}
