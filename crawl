#!/usr/bin/perl -s
#
# Perl script to crawl specified websites and return matches found
#
# By Andy Holyer 05/10/12
#
# usage: crawl "String" <site ...>
#

use warnings;
use WWW::Mechanize;
#use Data::Dumper;
# Variables we can tweak to change behaviour
my $DEBUG = 1;
my $MAXDEPTH = 10;

my $string = shift(@ARGV);
my @URLS;
my %visited = ();
my $mech = WWW::Mechanize->new();

my %IGNORE_SCHEME = ( 
    'mailto' => 1,
    'ftp'    => 1,
    'javascript' => 1,
);    

# Populate the initial URL list
foreach my $url (@ARGV) {
    push @URLS, { url =>$url, 
		  depth => 0
    };
}

while (my $loc = shift(@URLS)) {
    crawl($string, $loc->{url}, $loc->{depth});
       }

sub crawl {
    my ($string, $url, $depth) = @_;

    # Are we at the max depth yet?
    if ($depth >= $MAXDEPTH) {
	print "*** At Maximum depth: $url\n" if $DEBUG;
	return;
    }

    # Check it's not on the kill list
    if ($visited{$url}) {
	print "*** Already visited $url\n" if $DEBUG;
	return;
    } else {
	$visited{$url} = 1;
    }

    $mech->get($url);

    # See if we match the query string
    my $content = $mech->content();
    my $matches = 0;
    $matches++  while ($content =~ /$string/ogi);

    if ( $DEBUG || ($matches > 0)) {
	print "$url, \"$string\", $matches, $depth\n";
    }

    # Add all the links to the URL list

    foreach my $link ($mech->links()) {
	
	# Not all types of URLS can be followed
	next if $IGNORE_SCHEME{$link->url_abs()->scheme};
	push @URLS, {
	    url => $link->url_abs()->as_string, 
	    depth =>$depth+1
	};
    }
}
